# -*- coding: utf-8 -*-
"""flaskdeploy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fQBDbQBOlOxQ-WR7UN3qA7pzmvETXcFh
"""


import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

import matplotlib.pyplot as plt

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory


import seaborn as sns
import re
import matplotlib.pyplot as plt
import missingno as ms

import seaborn as sns

import nltk
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import TfidfVectorizer

import nltk

nltk.download('stopwords')

from sklearn import metrics

from sklearn.metrics import accuracy_score, classification_report

from sklearn.linear_model import LogisticRegression

import warnings

warnings.filterwarnings("ignore")

training_data = pd.read_csv(
    '../hatespeech2/train_E6oV3lV.csv')  # to read and store in panda dataframe

nltk.download('stopwords')
eng_stops = set(stopwords.words("english"))

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()


def process_message(review_text):
    # remove all the special characters
    new_review_text = re.sub("[^a-zA-Z]", " ", review_text)
    # convert all letters to lower case
    words = new_review_text.lower().split()
    # remove stop words
    words = [w for w in words if not w in eng_stops]
    # lemmatizer
    words = [lemmatizer.lemmatize(word) for word in words]
    # join all words back to text
    return " ".join(words)


import nltk
nltk.download('wordnet')

training_data['clean_tweet'] = training_data['tweet'].apply(lambda x: process_message(x))

training_data.head()

x = training_data['clean_tweet']
y = training_data['label']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

tfvect = TfidfVectorizer()
tfid_x_train = tfvect.fit_transform(x_train)
tfid_x_test = tfvect.transform(x_test)

classifier = LogisticRegression()
classifier.fit(tfid_x_train, y_train)

y_pred = classifier.predict(tfid_x_test)
score = accuracy_score(y_test, y_pred)
print(f'Accuracy: {round(score * 100, 2)}%')

cf = confusion_matrix(y_test, y_pred, labels=[0,1])
print(cf)


def hate_speech_predictor(speech):
    input_data = [speech]
    vectorized_input_data = tfvect.transform(input_data)
    prediction = classifier.predict(vectorized_input_data)
    print(prediction)


hate_speech_predictor("user user lumpy says i am a . prove it lumpy.")


import pickle
pickle.dump(classifier, open('model.pkl', 'wb'))
